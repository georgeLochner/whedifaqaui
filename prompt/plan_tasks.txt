# Task Planning Prompt

You are planning implementation tasks for a specific feature. Your job is to decompose the feature into well-scoped tasks that coding agents can complete.

## Your Task

1. Read and understand the feature description
2. Read referenced design documentation
3. Create implementation tasks with appropriate scope
4. Set up task dependencies for sequential execution

## Step 1: Understand the Feature

Run `bd show <feature-id>` to read the full feature description. This contains:
- What the feature delivers
- Files to create/modify
- Technical context and design decisions
- Acceptance criteria

Also read any design docs referenced in the feature description.

## Step 2: Task Sizing Rules

Each task must be scoped so a coding agent can complete it within context limits.

### Hard Constraints

| Constraint | Limit | Rationale |
|------------|-------|-----------|
| Files modified | 1-3 | More files = more context = higher failure risk |
| Files read for context | 5-8 max | Reading files consumes context budget |
| Deliverable | Exactly 1 | Clear completion criteria |
| Description length | 3-4 sentences | If longer, task is too complex |

### Task Should Have

- **Clear deliverable**: "Create X that does Y" not "work on X"
- **Verifiable completion**: How do you know it's done? (tests pass, endpoint responds, component renders)
- **No exploration**: Research belongs in feature planning, not task execution
- **Specific file references**: Name the exact files to create or modify

### Task Should NOT Have

- Ambiguous scope ("improve", "refactor", "clean up")
- Multiple unrelated deliverables
- Dependency on discovering how things work
- Need to read more than 8 files

### Examples

**Good task**:
> Create the Video SQLAlchemy model in `backend/app/models/video.py` with fields: id (UUID), title, file_path, status, created_at. Add the model to `backend/app/models/__init__.py`. Verify by running alembic revision.

**Bad task** (too big):
> Implement the video upload feature including the model, service, API endpoint, and frontend form.

**Bad task** (too vague):
> Set up the database stuff.

**Bad task** (requires exploration):
> Figure out how to integrate WhisperX and implement it.

## Step 3: Identify Natural Task Boundaries

Look at the feature's "Files to Create" and "Files to Modify" lists. Common patterns:

### Backend Feature
1. Create/update database model(s)
2. Create Pydantic schemas
3. Implement service layer logic
4. Create API endpoint(s)
5. Write unit tests
6. Run integration tests (E2E verification)

### Frontend Feature
1. Create TypeScript types
2. Create API client functions
3. Implement component(s)
4. Create page and wire up routing
5. Write unit tests
6. Run E2E verification (Playwright)

### Processing/Pipeline Feature
1. Implement core logic (service)
2. Create Celery task wrapper
3. Wire up task triggers
4. Write unit tests
5. Run integration tests (E2E verification)

### Infrastructure Feature
1. Create configuration
2. Set up connections/clients
3. Write migrations
4. Verify with smoke test

## Step 3b: Mandatory Testing Tasks

Every feature MUST include testing tasks. Tests are the agent's feedback loop to verify implementation correctness.

### Testing Task Requirements

Each implementation task should specify which tests to run for verification. Additionally, create dedicated testing tasks:

1. **Unit test task(s)**: Write and run tests for individual components
   - Reference specific test IDs from the test spec (e.g., V1-U01, V1-U02)
   - Include the pytest/vitest command to run

2. **Integration/E2E verification task** (MUST be the final task): Verify the feature works end-to-end
   - Gates feature completion
   - The task description MUST say: `Read prompt/e2e_verification.txt for instructions.`
   - List the parent feature ID so the execution agent can cross-check
   - List the test spec location and relevant test IDs
   - **Copy every E2E scenario from the parent feature's Testing Requirements verbatim** — do not omit or reinterpret them. If the feature says "E2E-01 (steps 1-5)", those steps go into this task as-is.
   - If this feature completes a cross-feature E2E chain (see below), include that scenario too

### Cross-Feature E2E Identification

Some E2E scenarios span multiple features. Check if this feature is the LAST dependency:

1. Read the test specification's E2E section
2. For each E2E scenario, note which user stories it requires
3. If this feature covers the LAST required story → include that full E2E scenario in the verification task

### E2E Verification Task Template

```markdown
## Objective
Verify [Feature Name] works end-to-end.

Read prompt/e2e_verification.txt for instructions.

Feature ID: <feature-id>

## Test Specification Reference
- Document: docs/testing/phase{N}-test-specification.md
- Sections: [relevant sections]
- Test IDs: [list ALL test IDs from parent feature's Testing Requirements]

## Within-Feature E2E Scenarios
[Copy verbatim from parent feature's Testing Requirements → E2E Scenarios section]

## Cross-Feature E2E Scenarios (if applicable)
[Only if this feature completes a multi-feature E2E chain]

## Tests to Run
[List exact pytest/vitest commands for all test files in this feature]
```

## Step 4: Create Tasks

For each task:

```bash
bd create --title="[Action] [Thing]" --type=task --priority=2 --parent=<feature-id> --description="[Full task description]"
```

### Task Description Template

```markdown
## Objective
[One sentence: what to create/modify and why]

## Files to Modify
- `path/to/file.py` - [what to add/change]

## Files to Read (Context)
- `path/to/reference.py` - [why you need to read this]

## Implementation Details
[Specific guidance - function signatures, patterns to follow, etc.]

## Acceptance Criteria
- [ ] [Specific verifiable criterion]
- [ ] [Another criterion]

## Verification

### Tests to Write
[If this task includes writing tests, specify which test IDs from the spec]

### Tests to Run
```bash
[Exact command to run tests, e.g.: cd backend && pytest tests/unit/test_video.py -v]
```
Expected: [What success looks like, e.g., "All 4 tests pass"]

### Verification Loop
The agent must:
1. Implement the code
2. Run the tests specified above
3. If tests fail: fix and re-run
4. Only mark task complete when tests pass
```

## Step 5: Set Up Dependencies

Tasks within a feature execute sequentially:

```bash
# Get the IDs of tasks you just created
# Assume: task-001, task-002, task-003

# First task blocked by this "Plan tasks" task
bd dep add task-001 <this-plan-task-id>

# Each subsequent task blocked by previous
bd dep add task-002 task-001
bd dep add task-003 task-002
```

This ensures:
1. When you close this "Plan tasks" task, the first implementation task becomes ready
2. Each task completes before the next begins

## Step 6: Verify Before Closing

Before closing this task, verify:

- [ ] All tasks created with clear descriptions
- [ ] Task count is reasonable (3-8 tasks typical)
- [ ] Each task modifies 1-3 files
- [ ] Each task has ONE clear deliverable
- [ ] Each task includes specific test commands to run
- [ ] Final task is E2E verification task with `Read prompt/e2e_verification.txt for instructions.`
- [ ] E2E task lists the parent feature ID and all test IDs from the feature's Testing Requirements
- [ ] E2E task copies ALL E2E scenarios from parent feature verbatim (not reinterpreted)
- [ ] Cross-feature E2E scenarios included if this feature completes them
- [ ] Dependencies set: first task blocked by this task, chain through to last
- [ ] No circular dependencies

Run `bd show <feature-id>` to see all children and verify the structure.

## Common Mistakes to Avoid

1. **Task too big**: If description needs more than 4 sentences, split it
2. **Missing context files**: Agent will waste context rediscovering what you already know
3. **Implicit dependencies**: If task B needs code from task A, make that explicit
4. **No test commands**: Every task must specify exact commands to verify it works
5. **Missing E2E task**: The final task MUST be E2E verification
6. **Vague acceptance criteria**: "It works" is not verifiable - specify test IDs and commands
7. **Forgetting cross-feature E2E**: Check if this feature unlocks any E2E scenarios
8. **Tests written but not run**: Tests are a feedback loop - agents must run them
9. **Reinterpreting parent E2E scope**: If the parent feature lists "E2E-01 (steps 1-5)", do NOT look at the full E2E-01 (12 steps) and conclude it requires other features. The parent already scoped it — carry it forward verbatim.

## Example: Video Upload API Feature

```bash
# Task 1: Database model
bd create --title="Create Video database model" --type=task --parent=beads-feat-002 \
  --description="## Objective
Create the Video SQLAlchemy model for storing video metadata.

## Files to Modify
- \`backend/app/models/video.py\` - Create new file with Video model
- \`backend/app/models/__init__.py\` - Export Video model

## Files to Read
- \`docs/design/data-model.md\` - Schema definition
- \`backend/app/core/database.py\` - Base model pattern

## Implementation Details
Fields: id (UUID, PK), title (str), file_path (str), status (enum), recording_date (date), participants (array), created_at, updated_at.
Use SQLAlchemy 2.0 style with mapped_column.

## Acceptance Criteria
- [ ] Model class defined with all fields
- [ ] Exported from models package
- [ ] Alembic migration generated successfully

## Verification
\`\`\`bash
cd backend && alembic revision --autogenerate -m 'add video model'
cd backend && alembic upgrade head
\`\`\`
Expected: Migration generates and applies without errors"
# Returns: beads-task-101

# Task 2: Pydantic schemas
bd create --title="Create Video Pydantic schemas" --type=task --parent=beads-feat-002 \
  --description="..."
# Returns: beads-task-102

# Task 3: Video service
bd create --title="Implement Video service" --type=task --parent=beads-feat-002 \
  --description="..."
# Returns: beads-task-103

# Task 4: API endpoints
bd create --title="Create Video API endpoints" --type=task --parent=beads-feat-002 \
  --description="..."
# Returns: beads-task-104

# Task 5: E2E Verification (FINAL TASK)
bd create --title="E2E verification for Video Upload API" --type=task --parent=beads-feat-002 \
  --description="## Objective
Verify Video Upload API works end-to-end.

Read prompt/e2e_verification.txt for instructions.

Feature ID: beads-feat-002

## Test Specification Reference
- Document: docs/testing/phase1-test-specification.md
- Sections: V1 (Video Upload), V3 (Processing Status)
- Test IDs: V1-U01 to V1-U04, V1-I01 to V1-I04, V1-F01 to V1-F04, V3-U01, V3-U02, V3-I02, V3-I03, V3-F01 to V3-F03

## Within-Feature E2E Scenarios
- E2E-01 (steps 1-5): Upload flow - navigate to upload, fill form, submit, observe progress, wait for status
- E2E-05: Error handling - upload corrupted file, observe error status

## Tests to Run
\`\`\`bash
docker compose exec backend pytest tests/unit/test_video_schema.py -v
docker compose exec backend pytest tests/integration/test_video_api.py -v
docker compose exec frontend npm test
\`\`\`"
# Returns: beads-task-105

# Set dependencies
bd dep add beads-task-101 <this-plan-task-id>  # First task waits for planning
bd dep add beads-task-102 beads-task-101       # Schemas after model
bd dep add beads-task-103 beads-task-102       # Service after schemas
bd dep add beads-task-104 beads-task-103       # API after service
bd dep add beads-task-105 beads-task-104       # E2E verification last
```

## After Creating All Tasks

Close this planning task:
```bash
bd close <this-plan-task-id>
```

This unblocks the first implementation task, and the coordinator will begin assigning work.
