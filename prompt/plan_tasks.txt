# Task Planning Prompt

You are planning implementation tasks for a specific feature. Your job is to decompose the feature into well-scoped tasks that coding agents can complete.

## Your Task

1. Read and understand the feature description
2. Read referenced design documentation
3. Create implementation tasks with appropriate scope
4. Set up task dependencies for sequential execution

## Step 1: Understand the Feature

Run `bd show <feature-id>` to read the full feature description. This contains:
- What the feature delivers
- Files to create/modify
- Technical context and design decisions
- Acceptance criteria

Also read any design docs referenced in the feature description.

## Step 2: Task Sizing Rules

Each task must be scoped so a coding agent can complete it within context limits.

### Hard Constraints

| Constraint | Limit | Rationale |
|------------|-------|-----------|
| Files modified | 1-3 | More files = more context = higher failure risk |
| Files read for context | 5-8 max | Reading files consumes context budget |
| Deliverable | Exactly 1 | Clear completion criteria |
| Description length | 3-4 sentences | If longer, task is too complex |

### Task Should Have

- **Clear deliverable**: "Create X that does Y" not "work on X"
- **Verifiable completion**: How do you know it's done? (tests pass, endpoint responds, component renders)
- **No exploration**: Research belongs in feature planning, not task execution
- **Specific file references**: Name the exact files to create or modify

### Task Should NOT Have

- Ambiguous scope ("improve", "refactor", "clean up")
- Multiple unrelated deliverables
- Dependency on discovering how things work
- Need to read more than 8 files

### Examples

**Good task**:
> Create the Video SQLAlchemy model in `backend/app/models/video.py` with fields: id (UUID), title, file_path, status, created_at. Add the model to `backend/app/models/__init__.py`. Verify by running alembic revision.

**Bad task** (too big):
> Implement the video upload feature including the model, service, API endpoint, and frontend form.

**Bad task** (too vague):
> Set up the database stuff.

**Bad task** (requires exploration):
> Figure out how to integrate WhisperX and implement it.

## Step 3: Identify Natural Task Boundaries

Look at the feature's "Files to Create" and "Files to Modify" lists. Common patterns:

### Backend Feature
1. Create/update database model(s) + verify with migration
2. Create Pydantic schemas + unit tests for validation
3. Implement service layer logic + unit tests for service functions
4. Create API endpoint(s) + integration tests for endpoints
5. E2E verification

### Frontend Feature
1. Create API client functions + TypeScript types
2. Implement component(s) + unit tests for each component
3. Create page, wire up routing + page-level tests
4. E2E verification (Playwright)

### Processing/Pipeline Feature
1. Implement core logic (service) + unit tests for service functions
2. Create Celery task wrapper + unit tests for task logic
3. Wire up task triggers + integration tests
4. E2E verification

### Infrastructure Feature
1. Create configuration
2. Set up connections/clients
3. Write migrations
4. Verify with smoke test

**Key principle**: Each implementation task writes AND tests its own code. Never defer unit tests to a separate task — the agent that writes the code must also prove it works. This ensures broken code is caught immediately, not 3 tasks later by a different agent.

## Step 3b: Mandatory Testing

Every implementation task MUST include unit tests for the code it creates. Tests are the agent's feedback loop — they verify correctness at the point of creation, not after the fact.

### The Rule: Write Code → Write Tests → Run Tests → Close Task

Each implementation task must:
1. Write the implementation code
2. Write unit tests for that code (referencing test IDs from the test spec)
3. Run the tests and verify they pass
4. Only then close the task

**Do NOT create separate "write unit tests" tasks.** Unit tests belong with the code they test. The agent that writes a service function is the best agent to test it — it has full context.

**Dedicated test tasks are only for**:
- Integration tests that span multiple components (written after all components exist)
- The E2E verification task (always the final task)

### Integration/E2E Verification Task

The **final task** of every feature is the E2E verification task. This is the only dedicated test task:
   - Gates feature completion
   - The task description MUST say: `Read prompt/e2e_verification.txt for instructions.`
   - List the parent feature ID so the execution agent can cross-check
   - List the test spec location and relevant test IDs
   - **Copy every E2E scenario from the parent feature's Testing Requirements verbatim** — do not omit or reinterpret them. If the feature says "E2E-01 (steps 1-5)", those steps go into this task as-is.
   - If this feature completes a cross-feature E2E chain (see below), include that scenario too

### Cross-Feature E2E Identification

Some E2E scenarios span multiple features. Check if this feature is the LAST dependency:

1. Read the test specification's E2E section
2. For each E2E scenario, note which user stories it requires
3. If this feature covers the LAST required story → include that full E2E scenario in the verification task

### E2E Verification Task Template

```markdown
## Objective
Verify [Feature Name] works end-to-end.

Read prompt/e2e_verification.txt for instructions.

Feature ID: <feature-id>

## Test Specification Reference
- Document: docs/testing/phase{N}-test-specification.md
- Sections: [relevant sections]
- Test IDs: [list ALL test IDs from parent feature's Testing Requirements]

## Within-Feature E2E Scenarios
[Copy verbatim from parent feature's Testing Requirements → E2E Scenarios section]

## Cross-Feature E2E Scenarios (if applicable)
[Only if this feature completes a multi-feature E2E chain]

## Tests to Run
[List exact pytest/vitest commands for all test files in this feature]
```

## Step 4: Create Tasks

For each task:

```bash
bd create --title="[Action] [Thing]" --type=task --priority=2 --parent=<feature-id> --description="[Full task description]"
```

### Task Description Template

```markdown
## Objective
[One sentence: what to create/modify and why]

## Files to Modify
- `path/to/file.py` - [what to add/change]

## Files to Read (Context)
- `path/to/reference.py` - [why you need to read this]

## Implementation Details
[Specific guidance - function signatures, patterns to follow, etc.]

## Acceptance Criteria
- [ ] [Specific verifiable criterion]
- [ ] [Another criterion]

## Unit Tests (MANDATORY for implementation tasks)
Write unit tests for the code created in this task:
- Test IDs: [specific test IDs from the spec, e.g., V1-U01, V1-U02]
- Test file: [path to test file, e.g., `backend/tests/unit/test_video_schema.py`]

## Verification
```bash
[Exact command to run tests, e.g.: docker compose exec backend pytest tests/unit/test_video_schema.py -v]
```
Expected: [What success looks like, e.g., "All 4 tests pass"]

### Verification Loop
The agent must:
1. Implement the code
2. Write unit tests for the code
3. Run the tests
4. If tests fail: fix and re-run
5. Only mark task complete when tests pass
```

## Step 5: Set Up Dependencies

Tasks within a feature execute sequentially:

```bash
# Get the IDs of tasks you just created
# Assume: task-001, task-002, task-003

# First task blocked by this "Plan tasks" task
bd dep add task-001 <this-plan-task-id>

# Each subsequent task blocked by previous
bd dep add task-002 task-001
bd dep add task-003 task-002
```

This ensures:
1. When you close this "Plan tasks" task, the first implementation task becomes ready
2. Each task completes before the next begins

## Step 6: Verify Before Closing

Before closing this task, verify:

- [ ] All tasks created with clear descriptions
- [ ] Task count is reasonable (3-8 tasks typical)
- [ ] Each task modifies 1-3 files
- [ ] Each task has ONE clear deliverable
- [ ] Each implementation task includes unit tests for the code it creates (not deferred to a separate task)
- [ ] Each task includes specific test commands to run
- [ ] Final task is E2E verification task with `Read prompt/e2e_verification.txt for instructions.`
- [ ] E2E task lists the parent feature ID and all test IDs from the feature's Testing Requirements
- [ ] E2E task copies ALL E2E scenarios from parent feature verbatim (not reinterpreted)
- [ ] Cross-feature E2E scenarios included if this feature completes them
- [ ] Dependencies set: first task blocked by this task, chain through to last
- [ ] No circular dependencies

Run `bd show <feature-id>` to see all children and verify the structure.

## Common Mistakes to Avoid

1. **Task too big**: If description needs more than 4 sentences, split it
2. **Missing context files**: Agent will waste context rediscovering what you already know
3. **Implicit dependencies**: If task B needs code from task A, make that explicit
4. **No test commands**: Every task must specify exact commands to verify it works
5. **Missing E2E task**: The final task MUST be E2E verification
6. **Vague acceptance criteria**: "It works" is not verifiable - specify test IDs and commands
7. **Forgetting cross-feature E2E**: Check if this feature unlocks any E2E scenarios
8. **Tests written but not run**: Tests are a feedback loop - agents must run them
9. **Reinterpreting parent E2E scope**: If the parent feature lists "E2E-01 (steps 1-5)", do NOT look at the full E2E-01 (12 steps) and conclude it requires other features. The parent already scoped it — carry it forward verbatim.
10. **Deferring unit tests to a separate task**: The agent that writes the code must also test it. A separate "write tests" task creates a gap where broken code goes undetected for multiple tasks. Unit tests belong with the implementation.

## Example: Video Upload API Feature

```bash
# Task 1: Database model + migration
bd create --title="Create Video database model" --type=task --parent=beads-feat-002 \
  --description="## Objective
Create the Video SQLAlchemy model for storing video metadata.

## Files to Modify
- \`backend/app/models/video.py\` - Create new file with Video model
- \`backend/app/models/__init__.py\` - Export Video model

## Files to Read
- \`docs/design/data-model.md\` - Schema definition
- \`backend/app/core/database.py\` - Base model pattern

## Implementation Details
Fields: id (UUID, PK), title (str), file_path (str), status (enum), recording_date (date), participants (array), created_at, updated_at.
Use SQLAlchemy 2.0 style with mapped_column.

## Acceptance Criteria
- [ ] Model class defined with all fields
- [ ] Exported from models package
- [ ] Alembic migration generated successfully

## Verification
\`\`\`bash
docker compose exec backend alembic revision --autogenerate -m 'add video model'
docker compose exec backend alembic upgrade head
\`\`\`
Expected: Migration generates and applies without errors"
# Returns: beads-task-101

# Task 2: Pydantic schemas + schema unit tests
bd create --title="Create Video schemas with unit tests" --type=task --parent=beads-feat-002 \
  --description="## Objective
Create Pydantic schemas for video create/response and write unit tests.

## Files to Modify
- \`backend/app/schemas/video.py\` - VideoCreate, VideoResponse schemas
- \`backend/tests/unit/test_video_schema.py\` - Schema validation tests

## Unit Tests (MANDATORY)
- V1-U01: test_video_schema_validation
- V1-U03: test_video_metadata_required_fields
- V1-U04: test_participants_array_parsing
- V3-U01: test_status_enum_values

## Verification
\`\`\`bash
docker compose exec backend pytest tests/unit/test_video_schema.py -v
\`\`\`
Expected: All 4 test IDs pass"
# Returns: beads-task-102

# Task 3: Video service + service unit tests
bd create --title="Implement Video service with unit tests" --type=task --parent=beads-feat-002 \
  --description="## Objective
Implement video CRUD service and write unit tests for service functions.

## Unit Tests (MANDATORY)
- V1-U02: test_video_file_extension_validation
- V3-U02: test_status_transition_validation

## Verification
\`\`\`bash
docker compose exec backend pytest tests/unit/test_video_schema.py tests/unit/test_video_service.py -v
\`\`\`
Expected: All tests pass including new service tests"
# Returns: beads-task-103

# Task 4: API endpoints + integration tests
bd create --title="Create Video API endpoints with integration tests" --type=task --parent=beads-feat-002 \
  --description="## Objective
Create upload, list, detail, and status API endpoints with integration tests.

## Unit Tests (MANDATORY)
- V1-I01: test_upload_creates_video_record
- V1-I02: test_upload_stores_file
- V1-I03: test_upload_triggers_processing_task
- V1-I04: test_upload_returns_video_id
- V3-I02: test_status_endpoint_returns_current
- V3-I03: test_error_status_with_message

## Verification
\`\`\`bash
docker compose exec backend pytest tests/integration/test_video_api.py -v
\`\`\`
Expected: All 6 integration tests pass"
# Returns: beads-task-104

# Task 5: E2E Verification (FINAL TASK)
bd create --title="E2E verification for Video Upload API" --type=task --parent=beads-feat-002 \
  --description="## Objective
Verify Video Upload API works end-to-end.

Read prompt/e2e_verification.txt for instructions.

Feature ID: beads-feat-002

## Test Specification Reference
- Document: docs/testing/phase1-test-specification.md
- Sections: V1 (Video Upload), V3 (Processing Status)
- Test IDs: V1-U01 to V1-U04, V1-I01 to V1-I04, V1-F01 to V1-F04, V3-U01, V3-U02, V3-I02, V3-I03, V3-F01 to V3-F03

## Within-Feature E2E Scenarios
- E2E-01 (steps 1-5): Upload flow - navigate to upload, fill form, submit, observe progress, wait for status
- E2E-05: Error handling - upload corrupted file, observe error status

## Tests to Run
\`\`\`bash
docker compose exec backend pytest tests/ -v
docker compose exec frontend npm test
\`\`\`"
# Returns: beads-task-105

# Set dependencies
bd dep add beads-task-101 <this-plan-task-id>  # First task waits for planning
bd dep add beads-task-102 beads-task-101       # Schemas after model
bd dep add beads-task-103 beads-task-102       # Service after schemas
bd dep add beads-task-104 beads-task-103       # API + integration tests after service
bd dep add beads-task-105 beads-task-104       # E2E verification last
```

## After Creating All Tasks

Close this planning task:
```bash
bd close <this-plan-task-id>
```

This unblocks the first implementation task, and the coordinator will begin assigning work.
