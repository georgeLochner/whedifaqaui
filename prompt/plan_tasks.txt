# Task Planning Prompt

You are planning implementation tasks for a specific feature. Your job is to decompose the feature into well-scoped tasks that coding agents can complete.

## Your Task

1. Read and understand the feature description
2. Read referenced design documentation
3. Create implementation tasks with appropriate scope
4. Set up task dependencies for sequential execution

## Step 1: Understand the Feature

Run `bd show <feature-id>` to read the full feature description. This contains:
- What the feature delivers
- Files to create/modify
- Technical context and design decisions
- Acceptance criteria

Also read any design docs referenced in the feature description.

## Step 2: Task Sizing Rules

Each task must be scoped so a coding agent can complete it within context limits.

### Hard Constraints

| Constraint | Limit | Rationale |
|------------|-------|-----------|
| Files modified | 1-3 | More files = more context = higher failure risk |
| Files read for context | 5-8 max | Reading files consumes context budget |
| Deliverable | Exactly 1 | Clear completion criteria |
| Description length | 3-4 sentences | If longer, task is too complex |

### Task Should Have

- **Clear deliverable**: "Create X that does Y" not "work on X"
- **Verifiable completion**: How do you know it's done? (tests pass, endpoint responds, component renders)
- **No exploration**: Research belongs in feature planning, not task execution
- **Specific file references**: Name the exact files to create or modify

### Task Should NOT Have

- Ambiguous scope ("improve", "refactor", "clean up")
- Multiple unrelated deliverables
- Dependency on discovering how things work
- Need to read more than 8 files

### Examples

**Good task**:
> Create the Video SQLAlchemy model in `backend/app/models/video.py` with fields: id (UUID), title, file_path, status, created_at. Add the model to `backend/app/models/__init__.py`. Verify by running alembic revision.

**Bad task** (too big):
> Implement the video upload feature including the model, service, API endpoint, and frontend form.

**Bad task** (too vague):
> Set up the database stuff.

**Bad task** (requires exploration):
> Figure out how to integrate WhisperX and implement it.

## Step 3: Identify Natural Task Boundaries

Look at the feature's "Files to Create" and "Files to Modify" lists. Common patterns:

### Backend Feature
1. Create/update database model(s)
2. Create Pydantic schemas
3. Implement service layer logic
4. Create API endpoint(s)
5. Write unit tests
6. Run integration tests (E2E verification)

### Frontend Feature
1. Create TypeScript types
2. Create API client functions
3. Implement component(s)
4. Create page and wire up routing
5. Write unit tests
6. Run E2E verification (Playwright)

### Processing/Pipeline Feature
1. Implement core logic (service)
2. Create Celery task wrapper
3. Wire up task triggers
4. Write unit tests
5. Run integration tests (E2E verification)

### Infrastructure Feature
1. Create configuration
2. Set up connections/clients
3. Write migrations
4. Verify with smoke test

## Step 3b: Mandatory Testing Tasks

Every feature MUST include testing tasks. Tests are the agent's feedback loop to verify implementation correctness.

### Testing Task Requirements

Each implementation task should specify which tests to run for verification. Additionally, create dedicated testing tasks:

1. **Unit test task(s)**: Write and run tests for individual components
   - Reference specific test IDs from the test spec (e.g., V1-U01, V1-U02)
   - Include the pytest/vitest command to run

2. **Integration/E2E verification task** (MUST be the final task): Verify the feature works end-to-end
   - Tests the maximum scope possible given what's implemented
   - References test IDs from the test spec
   - Includes exact commands to run tests
   - Gates feature completion
   - **MUST include every E2E scenario listed in the parent feature's Testing Requirements** — do not omit or reinterpret them. If the feature says "E2E-01 (steps 1-5)", those steps go into this task verbatim.

### E2E Scope by Feature Position

The E2E verification task should test as much of the stack as exists:

| Feature Type | E2E Scope |
|--------------|-----------|
| Database/Models | Verify schema, test CRUD operations |
| Backend Services | Service-level integration with DB |
| Backend API | API-level E2E: HTTP → DB → response |
| Frontend Components | Component tests with mocked API |
| Frontend Pages (full stack) | Playwright: browser → API → DB → UI |

### E2E Verification Task Template

```markdown
## Objective
Verify [Feature Name] works end-to-end at [scope level].

## Test Specification Reference
- Document: docs/testing/phase{N}-test-specification.md
- Section: [relevant section]
- Test IDs: [list specific test IDs]

## Tests to Run

### Unit Tests
\`\`\`bash
cd backend && pytest tests/unit/test_[module].py -v
\`\`\`
Expected: All [X] tests pass

### Integration Tests
\`\`\`bash
cd backend && pytest tests/integration/test_[feature].py -v
\`\`\`
Expected: All [X] tests pass

### E2E Tests (if applicable)
\`\`\`bash
cd e2e && npx playwright test [spec].spec.ts
\`\`\`
Expected: Scenario passes, screenshots captured

## Acceptance Criteria
- [ ] All unit tests passing
- [ ] All integration tests passing
- [ ] E2E scenario passes (if applicable)
- [ ] No regressions in existing tests
```

## Step 3c: E2E Scenarios

There are two types of E2E scenarios to include in the final verification task.

### Within-Feature E2E (from parent feature description)

The parent feature's Testing Requirements section lists E2E scenarios that this feature is responsible for. These are **authoritative and mandatory** — copy them into the E2E verification task verbatim.

**IMPORTANT**: The parent feature may scope an E2E scenario to specific steps (e.g., "E2E-01 (steps 1-5)"). This means those steps are testable with this feature alone. Do NOT dismiss them by looking at the full E2E scenario and concluding it requires other features. The parent feature has already done that scoping for you.

For each within-feature E2E scenario, the verification task must:
1. List the exact steps from the test specification
2. Include commands to execute them (Playwright, curl, or manual verification)
3. Include acceptance criteria for each step

### Cross-Feature E2E (completing multi-feature chains)

Some E2E scenarios span multiple features. When planning tasks, check if this feature completes the chain for any cross-feature E2E scenario.

#### How to Identify Cross-Feature E2E Tests

1. Read the test specification's E2E section
2. For each E2E scenario, note which user stories it requires
3. Check if this feature covers the LAST required user story
4. If yes → include that E2E scenario in this feature's final testing task

#### Example

E2E-01 "Upload-to-search flow" requires: V1, V2, V3, S1

- Feature A covered: V1, V2, V3 (Upload + Processing)
- Feature B covers: S1 (Search)
- Feature B is the LAST dependency for E2E-01
- Therefore: Feature B's E2E task must include E2E-01

### E2E Task Template Addition

Include both types in the E2E verification task:

```markdown
## Within-Feature E2E Scenarios

From parent feature Testing Requirements:
- E2E-XX (steps N-M): [scenario name]
  - Step N: [action] → verify [expected result]
  - Step N+1: [action] → verify [expected result]
  - ...

### Run Within-Feature E2E
\`\`\`bash
[exact commands — Playwright, curl, or manual verification steps]
\`\`\`
Expected: All steps pass

## Cross-Feature E2E Scenarios (if applicable)

This feature completes the requirements for:
- E2E-XX: [scenario name]
  - Required stories: [list]
  - All prerequisites now complete

### Run Cross-Feature E2E
\`\`\`bash
cd e2e && npx playwright test e2e-xx.spec.ts
\`\`\`
Expected: Full scenario passes
```

## Step 4: Create Tasks

For each task:

```bash
bd create --title="[Action] [Thing]" --type=task --priority=2 --parent=<feature-id> --description="[Full task description]"
```

### Task Description Template

```markdown
## Objective
[One sentence: what to create/modify and why]

## Files to Modify
- `path/to/file.py` - [what to add/change]

## Files to Read (Context)
- `path/to/reference.py` - [why you need to read this]

## Implementation Details
[Specific guidance - function signatures, patterns to follow, etc.]

## Acceptance Criteria
- [ ] [Specific verifiable criterion]
- [ ] [Another criterion]

## Verification

### Tests to Write
[If this task includes writing tests, specify which test IDs from the spec]

### Tests to Run
```bash
[Exact command to run tests, e.g.: cd backend && pytest tests/unit/test_video.py -v]
```
Expected: [What success looks like, e.g., "All 4 tests pass"]

### Verification Loop
The agent must:
1. Implement the code
2. Run the tests specified above
3. If tests fail: fix and re-run
4. Only mark task complete when tests pass
```

## Step 5: Set Up Dependencies

Tasks within a feature execute sequentially:

```bash
# Get the IDs of tasks you just created
# Assume: task-001, task-002, task-003

# First task blocked by this "Plan tasks" task
bd dep add task-001 <this-plan-task-id>

# Each subsequent task blocked by previous
bd dep add task-002 task-001
bd dep add task-003 task-002
```

This ensures:
1. When you close this "Plan tasks" task, the first implementation task becomes ready
2. Each task completes before the next begins

## Step 6: Verify Before Closing

Before closing this task, verify:

- [ ] All tasks created with clear descriptions
- [ ] Task count is reasonable (3-8 tasks typical)
- [ ] Each task modifies 1-3 files
- [ ] Each task has ONE clear deliverable
- [ ] Each task includes specific test commands to run
- [ ] Final task is E2E verification task
- [ ] E2E task references test spec IDs
- [ ] E2E task includes ALL E2E scenarios from parent feature's Testing Requirements (verbatim, not reinterpreted)
- [ ] Cross-feature E2E scenarios included if this feature completes them
- [ ] Dependencies set: first task blocked by this task, chain through to last
- [ ] No circular dependencies

Run `bd show <feature-id>` to see all children and verify the structure.

## Common Mistakes to Avoid

1. **Task too big**: If description needs more than 4 sentences, split it
2. **Missing context files**: Agent will waste context rediscovering what you already know
3. **Implicit dependencies**: If task B needs code from task A, make that explicit
4. **No test commands**: Every task must specify exact commands to verify it works
5. **Missing E2E task**: The final task MUST be E2E verification
6. **Vague acceptance criteria**: "It works" is not verifiable - specify test IDs and commands
7. **Forgetting cross-feature E2E**: Check if this feature unlocks any E2E scenarios
8. **Tests written but not run**: Tests are a feedback loop - agents must run them
9. **Reinterpreting parent E2E scope**: If the parent feature lists "E2E-01 (steps 1-5)", do NOT look at the full E2E-01 (12 steps) and conclude it requires other features. The parent already scoped it — carry it forward verbatim.

## Example: Video Upload API Feature

```bash
# Task 1: Database model
bd create --title="Create Video database model" --type=task --parent=beads-feat-002 \
  --description="## Objective
Create the Video SQLAlchemy model for storing video metadata.

## Files to Modify
- \`backend/app/models/video.py\` - Create new file with Video model
- \`backend/app/models/__init__.py\` - Export Video model

## Files to Read
- \`docs/design/data-model.md\` - Schema definition
- \`backend/app/core/database.py\` - Base model pattern

## Implementation Details
Fields: id (UUID, PK), title (str), file_path (str), status (enum), recording_date (date), participants (array), created_at, updated_at.
Use SQLAlchemy 2.0 style with mapped_column.

## Acceptance Criteria
- [ ] Model class defined with all fields
- [ ] Exported from models package
- [ ] Alembic migration generated successfully

## Verification
\`\`\`bash
cd backend && alembic revision --autogenerate -m 'add video model'
cd backend && alembic upgrade head
\`\`\`
Expected: Migration generates and applies without errors"
# Returns: beads-task-101

# Task 2: Pydantic schemas
bd create --title="Create Video Pydantic schemas" --type=task --parent=beads-feat-002 \
  --description="..."
# Returns: beads-task-102

# Task 3: Video service
bd create --title="Implement Video service" --type=task --parent=beads-feat-002 \
  --description="..."
# Returns: beads-task-103

# Task 4: API endpoints
bd create --title="Create Video API endpoints" --type=task --parent=beads-feat-002 \
  --description="..."
# Returns: beads-task-104

# Task 5: E2E Verification (FINAL TASK)
bd create --title="E2E verification for Video Upload API" --type=task --parent=beads-feat-002 \
  --description="## Objective
Verify Video Upload API works end-to-end.

## Test Specification Reference
- Document: docs/testing/phase1-test-specification.md
- Section: V1 (Video Upload)
- Test IDs: V1-U01 to V1-U04, V1-I01 to V1-I04

## Tests to Run

### Unit Tests
\`\`\`bash
cd backend && pytest tests/unit/test_video_schema.py -v
\`\`\`
Expected: All V1-U tests pass

### Integration Tests
\`\`\`bash
cd backend && pytest tests/integration/test_video_api.py -v
\`\`\`
Expected: All V1-I tests pass

## Acceptance Criteria
- [ ] All unit tests passing
- [ ] All integration tests passing
- [ ] POST /videos creates record and returns ID
- [ ] GET /videos returns list
- [ ] GET /videos/{id}/status returns current status
- [ ] No regressions in existing tests"
# Returns: beads-task-105

# Set dependencies
bd dep add beads-task-101 <this-plan-task-id>  # First task waits for planning
bd dep add beads-task-102 beads-task-101       # Schemas after model
bd dep add beads-task-103 beads-task-102       # Service after schemas
bd dep add beads-task-104 beads-task-103       # API after service
bd dep add beads-task-105 beads-task-104       # E2E verification last
```

## After Creating All Tasks

Close this planning task:
```bash
bd close <this-plan-task-id>
```

This unblocks the first implementation task, and the coordinator will begin assigning work.
