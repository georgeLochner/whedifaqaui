{"id":"w-0ta","title":"Test Phase 1","description":"Test","status":"tombstone","priority":1,"issue_type":"task","created_at":"2026-02-07T13:44:38.876397463+02:00","updated_at":"2026-02-07T13:44:59.894121324+02:00","deleted_at":"2026-02-07T13:44:59.894121324+02:00","deleted_by":"daemon","delete_reason":"delete","original_type":"task"}
{"id":"w-3vg","title":"Phase 1 Regression Testing","description":"# Feature: Phase 1 Regression Testing\n\n## Overview\nExecute the full regression test pack to verify Phase 1 functionality and ensure no regressions from prior work. This is the final quality gate before the phase is considered complete.\n\n## Regression Pack Location\ndocs/testing/regression-pack.md\n\n## What This Feature Does\n- Executes all regression tests for Phase 1 (R1-01 through R1-10)\n- Verifies expected outcomes for each test\n- Reports PASS/FAIL for each test\n- Blocks phase completion until all tests pass\n\n## Testing Requirements\n- All tests in regression pack Phase 1 section must pass (R1-01 through R1-10)\n- Any failures must be fixed before closing\n- Results must be documented\n\n## Reference Documentation\n- docs/testing/regression-pack.md - Test scenarios to execute\n- docs/testing/phase1-test-specification.md - Detailed acceptance criteria","status":"open","priority":2,"issue_type":"feature","created_at":"2026-02-07T13:59:05.050757932+02:00","updated_at":"2026-02-07T13:59:05.050757932+02:00"}
{"id":"w-3vg.1","title":"Plan tasks for Phase 1 Regression Testing","description":"Read prompt/plan_tasks.txt for instructions. Then create implementation tasks for this feature.\n\nFeature ID: w-3vg\n\n## Context\nExecute all regression tests and verify Phase 1 functionality. This is the final quality gate for the phase.\n\n## Sizing Guidance\nThis feature should result in a single task:\n\n## Objective\nExecute all regression tests and verify Phase 1 functionality.\n\n## Instructions\n1. Read docs/testing/regression-pack.md\n2. Execute each test scenario in \"Phase 1 Tests\" (R1-01 through R1-10)\n3. For each test:\n   - Follow the steps exactly\n   - Verify expected outcomes\n   - Record PASS or FAIL with details\n4. If any test fails:\n   - Document the failure in detail\n   - Do NOT close this task\n   - Create a blocking bug task to fix the issue\n5. Only close when ALL regression tests pass\n\n## Report Format\nFor each test, output:\n- Test ID: PASS/FAIL\n- Details: [what was observed]\n\n## Acceptance Criteria\n- [ ] All regression tests executed\n- [ ] All tests PASS\n- [ ] Results documented\n\n## Reference\nSee the feature description (bd show w-3vg) for full technical context.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T13:59:14.957472108+02:00","updated_at":"2026-02-07T13:59:14.957472108+02:00","dependencies":[{"issue_id":"w-3vg.1","depends_on_id":"w-3vg","type":"parent-child","created_at":"2026-02-07T13:59:14.96124354+02:00","created_by":"daemon"},{"issue_id":"w-3vg.1","depends_on_id":"w-ych","type":"blocks","created_at":"2026-02-07T13:59:31.706720444+02:00","created_by":"daemon"}]}
{"id":"w-7ib","title":"Project Scaffolding \u0026 Infrastructure","description":"# Feature: Project Scaffolding \u0026 Infrastructure\n\n## Overview\nSet up the foundational project structure, Docker Compose services, database models, migrations, and configuration. This feature creates the skeleton that all subsequent features build upon — including the backend FastAPI app, frontend React app, database schema, and Docker infrastructure.\n\n## User Stories Covered\n- V3 (partial): Status enum and model definition\n- Infrastructure foundation for all stories\n\n## Technical Context\n\n### Files to Create\n- `docker-compose.yml` - All infrastructure services (postgres, opensearch, redis, backend, celery-worker, frontend)\n- `.env.example` - Environment variable template\n- `backend/Dockerfile` - Python 3.11 + FFmpeg + system deps\n- `backend/requirements.txt` - Phase 1 Python dependencies\n- `backend/alembic.ini` - Alembic configuration\n- `backend/app/__init__.py` - Package init\n- `backend/app/main.py` - FastAPI application entry with health endpoint and CORS\n- `backend/app/core/__init__.py` - Core package init\n- `backend/app/core/config.py` - Settings via pydantic-settings (DATABASE_URL, OPENSEARCH_URL, REDIS_URL, etc.)\n- `backend/app/core/database.py` - SQLAlchemy engine, sessionmaker, Base, get_db dependency\n- `backend/app/core/opensearch.py` - OpenSearch client singleton\n- `backend/app/models/__init__.py` - Models package init\n- `backend/app/models/video.py` - Video SQLAlchemy model (id UUID, title, file_path, processed_path, thumbnail_path, duration, recording_date, participants[], context_notes, status, error_message, created_at, updated_at)\n- `backend/app/models/transcript.py` - Transcript model (id UUID, video_id FK, full_text, language, word_count, created_at; UNIQUE on video_id)\n- `backend/app/models/segment.py` - Segment model (id UUID, transcript_id FK, video_id FK, start_time, end_time, text, speaker, chunking_method, created_at; index on speaker)\n- `backend/app/schemas/__init__.py` - Schemas package init\n- `backend/app/schemas/video.py` - VideoCreate, VideoResponse, VideoStatusResponse Pydantic schemas\n- `backend/app/api/__init__.py` - API package init\n- `backend/app/api/deps.py` - Dependency injection (get_db, get_opensearch)\n- `backend/app/api/routes/__init__.py` - Routes package init\n- `backend/app/tasks/__init__.py` - Tasks package init\n- `backend/app/tasks/celery_app.py` - Celery configuration with Redis broker\n- `backend/app/services/__init__.py` - Services package init\n- `backend/app/migrations/` - Alembic migrations directory with env.py\n- `backend/app/migrations/versions/` - Migration versions directory\n- `backend/tests/__init__.py` - Tests package\n- `backend/tests/conftest.py` - Shared test fixtures (test DB session, test client)\n- `frontend/package.json` - Phase 1 dependencies (react, react-dom, react-router-dom, axios, video.js, typescript, vite, tailwindcss, vitest, @testing-library/react)\n- `frontend/Dockerfile` - Node.js build + serve\n- `frontend/vite.config.ts` - Vite configuration with proxy to backend\n- `frontend/tailwind.config.js` - Tailwind configuration\n- `frontend/tsconfig.json` - TypeScript configuration\n- `frontend/index.html` - HTML entry point\n- `frontend/src/main.tsx` - React entry point\n- `frontend/src/App.tsx` - Router setup with routes for /, /upload, /videos/:id, /search\n- `frontend/src/api/client.ts` - Axios instance configured with base URL\n- `frontend/src/styles/index.css` - Tailwind imports\n- `frontend/src/types/video.ts` - Video, Transcript, Segment TypeScript types\n- `data/` - Directory structure for videos/original, videos/processed, videos/audio, videos/thumbnails, transcripts, temp\n\n### Key Design Decisions\n- PostgreSQL 16 as source of truth, OpenSearch as derived search index\n- UUID primary keys for all models (gen_random_uuid)\n- Video status enum: uploaded, processing, transcribing, chunking, indexing, ready, error\n- Celery + Redis for async task processing\n- pydantic-settings for configuration management\n- Alembic for database migrations\n- CORS enabled for frontend dev server (localhost:3000)\n\n## Implementation Notes\n- Use SQLAlchemy 2.0 style (mapped_column, DeclarativeBase)\n- Status field is VARCHAR(50) not a DB enum (easier to extend)\n- participants stored as TEXT[] (PostgreSQL array)\n- recording_date is DATE type (not timestamp)\n- Frontend uses Vite dev server with proxy to backend API at localhost:8000\n- Health endpoint: GET /api/health returns {\"status\": \"ok\"}\n- Generate initial Alembic migration after models are defined\n- Docker Compose uses healthchecks on postgres, opensearch, redis\n- Backend depends_on with condition: service_healthy\n- GPU support in Docker Compose for backend and celery-worker (nvidia driver)\n\n## Testing Requirements\n\n### Test Specification Reference\n- Document: docs/testing/phase1-test-specification.md\n- Sections: Test Environment, Test Data Management\n\n### Unit Tests\n- V1-U01: test_video_schema_validation - Validate VideoCreate schema\n- V3-U01: test_status_enum_values - Valid status values defined\n- V3-U02: test_status_transition_validation - Invalid transitions rejected\n\n### Integration Tests\n- Health endpoint returns 200 with {\"status\": \"ok\"}\n- Database connection works (session can query)\n- Alembic migrations run cleanly (upgrade/downgrade)\n\n### Quality Gate\nThis feature is NOT complete until:\n- All models defined and migrations generated\n- Docker Compose starts all services\n- Health endpoint responds\n- Test fixtures (conftest.py) work for subsequent features\n- Frontend dev server runs and renders placeholder App\n\n## Reference Documentation\n- docs/design/technology-stack.md - All versions and packages\n- docs/design/data-model.md - PostgreSQL schema\n- docs/design/deployment.md - Docker Compose configuration\n- docs/implementation/phase1.md - Project structure, models, infrastructure","status":"open","priority":2,"issue_type":"feature","created_at":"2026-02-07T13:53:47.249110395+02:00","updated_at":"2026-02-07T13:53:47.249110395+02:00"}
{"id":"w-7ib.1","title":"Plan tasks for Project Scaffolding \u0026 Infrastructure","description":"Read prompt/plan_tasks.txt for instructions. Then create implementation tasks for this feature.\n\nFeature ID: w-7ib\n\n## Context\nSet up the foundational project structure, Docker Compose services, database models, migrations, and configuration. This creates the skeleton that all subsequent features build upon — backend FastAPI app, frontend React app, database schema, and Docker infrastructure.\n\n## Sizing Guidance\nTasks should:\n- Modify 1-3 files\n- Read no more than 5-8 files for context\n- Have ONE clear deliverable\n- Be completable within ~50k tokens of context\n\n## Reference\nSee the feature description (bd show w-7ib) for full technical context.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T13:53:56.156803196+02:00","updated_at":"2026-02-07T13:53:56.156803196+02:00","dependencies":[{"issue_id":"w-7ib.1","depends_on_id":"w-7ib","type":"parent-child","created_at":"2026-02-07T13:53:56.16181679+02:00","created_by":"daemon"}]}
{"id":"w-jjb","title":"Video Playback \u0026 Transcript UI","description":"# Feature: Video Playback \u0026 Transcript UI\n\n## Overview\nImplement the video playback experience: backend streaming endpoint (range request support), the VideoPage with embedded player (Video.js), synchronized transcript panel, and timestamp navigation (clickable timestamps + URL deep-linking). This is the primary video consumption interface where users watch videos and follow along with transcripts.\n\n## User Stories Covered\n- P1: Embedded video player with standard controls\n- P2: Timestamp navigation (clickable timestamps, URL deep-linking)\n- P3: Synchronized transcript display with speaker labels\n\n## Technical Context\n\n### Files to Create\n- `backend/app/api/routes/playback.py` - GET /api/videos/{id}/stream (video file with range request support), GET /api/videos/{id}/transcript (transcript segments as JSON)\n- `frontend/src/pages/VideoPage.tsx` - Video page layout: player + transcript panel side-by-side\n- `frontend/src/components/video/VideoPlayer.tsx` - Video.js player component, plays from /api/videos/{id}/stream, supports seeking via prop\n- `frontend/src/components/video/TranscriptPanel.tsx` - Scrollable transcript: list of segments with speaker labels, highlights active segment, click to seek, auto-scrolls\n- `frontend/src/components/video/TimestampLink.tsx` - Clickable timestamp component: displays formatted time (MM:SS), onClick seeks video or navigates to /videos/{id}?t=seconds\n- `frontend/src/hooks/useVideoPlayer.ts` - Custom hook: manages player ref, currentTime state, seek function, handles ?t= URL parameter on mount\n- `frontend/src/hooks/useTranscriptSync.ts` - Custom hook: given currentTime and segments, returns activeSegmentIndex\n- `frontend/src/types/transcript.ts` - Transcript and Segment TypeScript types\n- `frontend/src/api/videos.ts` - (extend) getTranscript(videoId) API call\n- `frontend/src/__tests__/components/VideoPlayer.test.tsx` - Player render and source tests\n- `frontend/src/__tests__/components/TranscriptPanel.test.tsx` - Transcript rendering, highlighting, click tests\n- `backend/tests/integration/test_playback_api.py` - Stream and transcript endpoint tests\n\n### Files to Modify\n- `backend/app/main.py` - Register playback routes\n- `backend/app/api/routes/__init__.py` - Export playback router\n- `frontend/src/App.tsx` - Wire up VideoPage route at /videos/:id\n\n### Dependencies\n- Feature 1 (Scaffolding): Base app, routing\n- Feature 2 (Upload): Video records exist, files on disk\n- Feature 3 (Pipeline): Processed MP4 exists, transcript/segments in DB\n- External: video.js (npm), react-router-dom (for URL params)\n\n### Key Design Decisions\n- Video.js for player (not native \u003cvideo\u003e) - provides consistent cross-browser controls\n- Stream endpoint serves processed MP4 file (not original MKV) with Range request support (HTTP 206)\n- Content-Type: video/mp4\n- Transcript endpoint returns all segments for the video, ordered by start_time\n- Each segment in transcript response: {id, start_time, end_time, text, speaker}\n- Active segment determined by comparing currentTime to segment start/end times\n- Auto-scroll uses scrollIntoView with smooth behavior\n- URL deep-linking: /videos/{id}?t=123 → on mount, seek player to 123 seconds\n- TimestampLink formats seconds as MM:SS (e.g., 125 → \"2:05\", 3661 → \"61:01\")\n- Transcript panel shows speaker labels as prefix: \"SPEAKER_00: text here...\"\n\n## Implementation Notes\n- Stream endpoint: use FileResponse with range support, or implement manually with StreamingResponse reading file chunks\n- Range request handling: parse Range header, return 206 with Content-Range\n- Video.js initialization: create player in useEffect, dispose on unmount\n- Video.js source: { src: '/api/videos/{id}/stream', type: 'video/mp4' }\n- Transcript sync: listen to 'timeupdate' event on video element, update currentTime state\n- Active segment: find segment where start_time \u003c= currentTime \u003c end_time\n- Click segment handler: call player.currentTime(segment.start_time)\n- URL timestamp: read searchParams.get('t') on mount, parse as float, seek\n- When user clicks timestamp on transcript, update URL with ?t= parameter (replaceState)\n- data-testid attributes: video-player, transcript-panel, transcript-segment, speaker-label, timestamp-link\n\n## Testing Requirements\n\n### Test Specification Reference\n- Document: docs/testing/phase1-test-specification.md\n- Sections: P1: Embedded Video Player, P2: Timestamp Navigation, P3: Synchronized Transcript\n\n### Unit Tests (Backend)\n- P2-U01: test_timestamp_formatting - 125 → \"2:05\"\n- P2-U02: test_timestamp_parsing - \"2:05\" → 125\n\n### Integration Tests (Backend)\n- P1-I01: test_stream_endpoint_returns_video - GET /api/videos/{id}/stream returns bytes\n- P1-I02: test_stream_supports_range_requests - Range header → 206 Partial Content\n- P1-I03: test_stream_content_type - Content-Type is video/mp4\n- P3-I01: test_transcript_endpoint_returns_segments - GET /api/videos/{id}/transcript returns segments with timestamps\n- P3-I02: test_transcript_includes_speaker - Speaker labels in response\n\n### Frontend Unit Tests\n- P1-F01: test_video_player_renders - \u003cvideo\u003e element present\n- P1-F02: test_player_controls_visible - Play, pause, seek, volume visible\n- P1-F03: test_player_loads_source - src points to /api/videos/{id}/stream\n- P2-F01: test_timestamp_link_renders - \"1:23\" visible\n- P2-F02: test_timestamp_click_seeks_video - Click sets currentTime\n- P2-F03: test_url_with_timestamp - URL includes ?t=seconds\n- P2-F04: test_url_timestamp_auto_seeks - Page load with ?t seeks player\n- P3-F01: test_transcript_panel_renders - Segments listed\n- P3-F02: test_current_segment_highlighted - Active segment has CSS class\n- P3-F03: test_segment_click_seeks - Click segment → player seeks to start_time\n- P3-F04: test_auto_scroll_to_current - Active segment scrolled into view\n- P3-F05: test_speaker_labels_displayed - \"SPEAKER_00:\" prefix visible\n\n### E2E Scenarios\n- E2E-01 (steps 10-12): Click timestamp from search, player loads and seeks, transcript syncs\n- E2E-02: Full playback with transcript sync, click transcript to seek, URL deep-linking\n\n### Quality Gate\nThis feature is NOT complete until:\n- All P1-*, P2-*, P3-* tests pass\n- Video plays in browser from stream endpoint\n- Transcript highlights sync with playback\n- Clicking transcript segment seeks video\n- URL ?t= parameter works for deep-linking\n- Speaker labels displayed in transcript\n\n## Reference Documentation\n- docs/design/data-model.md - Transcript and Segment schemas\n- docs/design/processing-pipeline.md - Transcoded MP4 location\n- docs/design/technology-stack.md - Video.js version\n- docs/implementation/phase1.md - Playback endpoints, frontend components\n- docs/testing/phase1-test-specification.md - P1, P2, P3 test specs","status":"open","priority":2,"issue_type":"feature","created_at":"2026-02-07T13:57:35.840157467+02:00","updated_at":"2026-02-07T13:57:35.840157467+02:00"}
{"id":"w-jjb.1","title":"Plan tasks for Video Playback \u0026 Transcript UI","description":"Read prompt/plan_tasks.txt for instructions. Then create implementation tasks for this feature.\n\nFeature ID: w-jjb\n\n## Context\nImplement the video playback experience: backend streaming endpoint with range request support, Video.js player, synchronized transcript panel with speaker labels, and timestamp navigation with URL deep-linking. Covers stories P1, P2, P3.\n\n## Sizing Guidance\nTasks should:\n- Modify 1-3 files\n- Read no more than 5-8 files for context\n- Have ONE clear deliverable\n- Be completable within ~50k tokens of context\n\n## Reference\nSee the feature description (bd show w-jjb) for full technical context.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T13:57:45.21879959+02:00","updated_at":"2026-02-07T13:57:45.21879959+02:00","dependencies":[{"issue_id":"w-jjb.1","depends_on_id":"w-jjb","type":"parent-child","created_at":"2026-02-07T13:57:45.222409732+02:00","created_by":"daemon"},{"issue_id":"w-jjb.1","depends_on_id":"w-wqm","type":"blocks","created_at":"2026-02-07T13:59:31.678225801+02:00","created_by":"daemon"}]}
{"id":"w-nxc","title":"Plan features for Phase 1","description":"Read prompt/plan_features.txt for instructions. Then create features and plan tasks for Phase 1.\n\n## Phase Parameters\n\n- **Phase**: phase1\n- **Implementation plan**: docs/implementation/phase1.md\n- **Test specification**: docs/testing/phase1-test-specification.md\n- **Design docs**: docs/design/\n- **Regression pack**: docs/testing/regression-pack.md\n\n## Summary\n\nPhase 1 delivers the MVP Core: Upload → Transcribe → Basic Search → Play at Timestamp\n\nUser stories covered: V1, V2, V3, P1, P2, P3, S1, S3, M1\n\n## Your Task\n\n1. Read the implementation plan and test specification\n2. Identify feature boundaries (expect 4-7 features)\n3. Create features with comprehensive descriptions\n4. Create a 'Plan tasks' child task for each feature\n5. Set up dependencies so features execute sequentially\n6. Include Regression Testing as the final feature\n\n## Key Guidance\n\n- Features should be vertical slices, not horizontal layers\n- Each feature should have 3-8 tasks when decomposed\n- Feature descriptions must be self-contained (agent context seeding)\n- Include testing requirements in each feature description\n- The Regression Testing feature gates phase completion\n\nSee prompt/plan_features.txt for detailed instructions and templates.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-02-07T13:24:41.797932096+02:00","updated_at":"2026-02-07T14:00:13.211365124+02:00","closed_at":"2026-02-07T14:00:13.211365124+02:00","close_reason":"Closed"}
{"id":"w-pi2","title":"Video Processing Pipeline","description":"# Feature: Video Processing Pipeline\n\n## Overview\nImplement the async Celery pipeline that processes uploaded videos through four stages: FFmpeg transcode (MKV→MP4 + audio extraction + thumbnail), WhisperX transcription with speaker diarization, semantic chunking with embeddings, and status tracking through each stage. This is the core backend intelligence that transforms raw video into searchable content.\n\n## User Stories Covered\n- V2: Automatic transcription with timestamps and speaker diarization\n- V3: Processing status indicators (status transitions through pipeline)\n- C1: Semantic chunking for search indexing\n- P1 (partial): FFmpeg transcode to MP4 for browser playback\n\n## Technical Context\n\n### Files to Create\n- `backend/app/services/ffmpeg.py` - FFmpeg operations: remux_to_mp4(input, output), extract_audio(input, output_wav), generate_thumbnail(input, output, timestamp=5)\n- `backend/app/services/transcription.py` - WhisperX integration: load_model(), transcribe(audio_path) → {segments with speaker labels, language, word_count}\n- `backend/app/services/chunking.py` - Semantic chunking: chunk_transcript(segments) → chunks with start/end times, text, speaker, using embedding similarity for boundary detection\n- `backend/app/services/embedding.py` - BGE model wrapper: load_model(), generate_embedding(text) → 768-dim vector, batch_embed(texts) → list of vectors\n- `backend/app/tasks/video_processing.py` - Celery task: remux/transcode MKV→MP4, extract audio WAV, generate thumbnail, update status to \"processing\"→\"transcribing\"\n- `backend/app/tasks/transcription.py` - Celery task: run WhisperX on audio, save transcript record, save WhisperX JSON to /data/transcripts/{video_id}.json, create segments in DB, update status to \"transcribing\"→\"chunking\"\n- `backend/app/tasks/chunking.py` - Celery task: read raw segments, apply semantic chunking, generate embeddings, save chunked segments to DB, update status to \"chunking\"→\"indexing\"\n- `backend/app/tasks/indexing.py` - Celery task: read segments from DB, bulk index to OpenSearch with embeddings, update status to \"indexing\"→\"ready\"\n- `backend/tests/unit/test_transcription.py` - WhisperX output parsing tests\n- `backend/tests/unit/test_chunking.py` - Semantic chunking logic tests\n- `backend/tests/unit/test_embedding.py` - Embedding generation tests\n- `backend/tests/integration/test_processing_pipeline.py` - Full pipeline integration tests\n- `scripts/prepare-test-data.sh` - Test data preparation script (download YouTube video, create test files)\n- `scripts/convert_srt_to_ground_truth.py` - SRT to ground truth JSON converter\n\n### Files to Modify\n- `backend/app/tasks/celery_app.py` - Register task modules\n- `backend/app/models/video.py` - May need status transition helper method\n- `backend/app/core/config.py` - Add WHISPER_MODEL, WHISPER_DEVICE, HF_TOKEN settings\n\n### Dependencies\n- Feature 1 (Scaffolding): Models, DB, Celery config\n- Feature 2 (Upload): Video records exist in DB, upload triggers pipeline\n- External: FFmpeg (system package), WhisperX (pip), pyannote.audio (pip + HuggingFace token), sentence-transformers (pip)\n\n### Key Design Decisions\n- Pipeline is a Celery chain: video_processing → transcription → chunking → indexing\n- Each task updates video status before starting its work\n- Error handling: if any task fails, set status to \"error\" with error_message\n- WhisperX provides both transcription AND speaker diarization (SPEAKER_00, SPEAKER_01, etc.)\n- Semantic chunking uses embedding cosine similarity to detect topic boundaries (not fixed size)\n- Chunk size target: 50-500 words\n- Chunks split at sentence boundaries\n- Each chunk preserves speaker attribution\n- Embeddings use BGE model (BAAI/bge-base-en-v1.5) producing 768-dim vectors\n- Audio extracted as WAV temporarily, deleted after transcription\n- WhisperX JSON output saved to /data/transcripts/{video_id}.json for LLM verification\n- Thumbnail generated at 5-second mark of video\n\n## Implementation Notes\n- Use ffmpeg-python library for FFmpeg operations\n- WhisperX API: whisperx.load_model(model, device), model.transcribe(audio), whisperx.load_align_model(), whisperx.align(), whisperx.DiarizationPipeline(), diarize_model(audio)\n- Semantic chunking algorithm: compute embeddings for each raw segment, calculate cosine similarity between consecutive segments, split where similarity drops below threshold\n- Tasks should be idempotent where possible (re-runnable on failure)\n- Status transitions must be atomic (use DB transaction)\n- Processing status flow: uploaded → processing → transcribing → chunking → indexing → ready\n- On error: status → error, error_message populated with descriptive text (not stack trace)\n- Celery task chaining: video_processing.s(video_id) | transcription.s() | chunking.s() | indexing.s()\n- For testing: mock WhisperX with deterministic output, test actual FFmpeg with small test video\n\n## Testing Requirements\n\n### Test Specification Reference\n- Document: docs/testing/phase1-test-specification.md\n- Sections: V2: Automatic Transcription, V3: Processing Status, C1: Semantic Chunking\n\n### Unit Tests\n- V2-U01: test_whisperx_output_parsing - Segments extracted correctly from WhisperX JSON\n- V2-U02: test_segment_timestamp_extraction - Start/end times as float seconds\n- V2-U03: test_speaker_label_extraction - \"SPEAKER_00\" format preserved\n- V2-U04: test_word_count_calculation - Word count matches expected\n- V3-U01: test_status_enum_values - All status values defined\n- V3-U02: test_status_transition_validation - Invalid transitions rejected\n- C1-U01: test_chunk_size_limits - Chunks within 50-500 words\n- C1-U02: test_chunk_preserves_speaker - Speaker label retained per chunk\n- C1-U03: test_chunk_timestamps_valid - start_time \u003c end_time\n- C1-U04: test_semantic_boundary_detection - Splits at natural breaks\n- P1-U01: test_video_transcode_to_mp4 - MP4 file created from MKV\n- P1-U02: test_processed_path_stored - DB record updated with processed_path\n\n### Integration Tests\n- V2-I01: test_transcription_creates_transcript_record - Transcript row created with video_id\n- V2-I02: test_transcription_creates_segments - Segment count matches WhisperX output\n- V2-I03: test_audio_extraction - WAV extracted from MKV\n- V2-I04: test_transcription_with_test_video - Full pipeline with test video, transcript matches expected\n- V2-I05: test_thumbnail_generated - Thumbnail file exists at thumbnail_path\n- V3-I01: test_status_updates_during_processing - Status progresses through all stages\n- V3-I03: test_error_status_with_message - Error captured with descriptive message\n- C1-I01: test_chunking_creates_segments - Segments stored in DB\n- C1-I03: test_chunk_embeddings_generated - 768-dim vector per segment\n\n### E2E Tests (LLM-verified)\n- V2-E01: test_transcription_content_accuracy - Content accuracy ≥85% vs ground truth\n- V2-E02: test_speaker_diarization - Speaker count matches, transitions reasonable\n- V2-E03: test_timestamp_alignment - Boundaries within ±2s\n- V2-E04: test_key_terms_preserved - ≥90% key terms found\n- V2-E05: test_overall_transcription_quality - Weighted score ≥80%\n\n### E2E Scenarios\n- E2E-01 (steps 5-6): Wait for processing, LLM verification\n- E2E-05: Corrupted video error handling\n- E2E-06: Multi-speaker diarization with test_meeting_long.mkv\n- E2E-07: LLM transcript verification\n\n### Quality Gate\nThis feature is NOT complete until:\n- All V2-*, V3-*, C1-*, P1-U01/U02 tests pass\n- Full pipeline runs on test video: upload → processing → transcribing → chunking → ready\n- WhisperX produces segments with speaker labels\n- Semantic chunking produces reasonably sized chunks\n- Status transitions tracked correctly\n- Error handling works for corrupted video\n\n## Reference Documentation\n- docs/design/processing-pipeline.md - Pipeline stages and data flow\n- docs/design/data-model.md - Transcript and Segment schemas\n- docs/design/technology-stack.md - WhisperX, pyannote, sentence-transformers versions\n- docs/testing/phase1-test-specification.md - V2, V3, C1 test specs, LLM verification\n- docs/reference/ - Any extended specifications","status":"open","priority":2,"issue_type":"feature","created_at":"2026-02-07T13:55:41.909351277+02:00","updated_at":"2026-02-07T13:55:41.909351277+02:00"}
{"id":"w-pi2.1","title":"Plan tasks for Video Processing Pipeline","description":"Read prompt/plan_tasks.txt for instructions. Then create implementation tasks for this feature.\n\nFeature ID: w-pi2\n\n## Context\nImplement the async Celery pipeline that processes uploaded videos through four stages: FFmpeg transcode, WhisperX transcription with speaker diarization, semantic chunking with embeddings, and status tracking. This is the core backend intelligence that transforms raw video into searchable content. Covers stories V2, V3, C1, and P1 (partial).\n\n## Sizing Guidance\nTasks should:\n- Modify 1-3 files\n- Read no more than 5-8 files for context\n- Have ONE clear deliverable\n- Be completable within ~50k tokens of context\n\n## Reference\nSee the feature description (bd show w-pi2) for full technical context.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T13:55:53.369009812+02:00","updated_at":"2026-02-07T13:55:53.369009812+02:00","dependencies":[{"issue_id":"w-pi2.1","depends_on_id":"w-pi2","type":"parent-child","created_at":"2026-02-07T13:55:53.373146974+02:00","created_by":"daemon"},{"issue_id":"w-pi2.1","depends_on_id":"w-zqu","type":"blocks","created_at":"2026-02-07T13:59:31.64940875+02:00","created_by":"daemon"}]}
{"id":"w-wqm","title":"Search \u0026 Indexing","description":"# Feature: Search \u0026 Indexing\n\n## Overview\nImplement the OpenSearch indexing and hybrid search (BM25 + vector/kNN) backend. This includes creating the OpenSearch index with proper mappings, the indexing service that bulk-indexes segments with embeddings, and the search API that accepts natural language queries, generates query embeddings, constructs hybrid queries, and returns ranked results with video IDs and timestamps.\n\n## User Stories Covered\n- S1: Natural language search (hybrid: BM25 + vector)\n- S3 (backend): Search results include video_id and timestamps\n\n## Technical Context\n\n### Files to Create\n- `backend/app/services/search.py` - Search service: create_index(), index_segments(segments), hybrid_search(query, limit) → ranked results with video_id, start_time, end_time, text, speaker, score\n- `backend/app/api/routes/search.py` - GET /api/search?q=...\u0026limit=10 → search results\n- `backend/app/schemas/search.py` - SearchQuery, SearchResult, SearchResponse Pydantic schemas\n- `backend/tests/unit/test_search.py` - Search query construction, result ranking tests\n- `backend/tests/integration/test_search_api.py` - Search API integration tests\n\n### Files to Modify\n- `backend/app/main.py` - Register search routes\n- `backend/app/api/routes/__init__.py` - Export search router\n- `backend/app/core/opensearch.py` - May need index management helpers\n- `backend/app/tasks/indexing.py` - Wire up to search service for bulk indexing\n\n### Dependencies\n- Feature 1 (Scaffolding): OpenSearch client, config\n- Feature 3 (Pipeline): Embedding service, segments with embeddings exist in DB\n- External: opensearch-py, sentence-transformers (for query embedding)\n\n### Key Design Decisions\n- OpenSearch index name: \"segments\" (or configurable via settings)\n- Index mapping: text (english analyzer), embedding (knn_vector 768-dim, HNSW, cosine), video_id, video_title, transcript_id, start_time, end_time, speaker, recording_date, created_at\n- KNN enabled on index: settings.index.knn = true\n- Hybrid search combines BM25 text match + kNN vector similarity\n- Query embedding generated at search time using same BGE model as indexing\n- Results ranked by combined score (Reciprocal Rank Fusion or weighted combination)\n- Empty query returns empty results (not error)\n- Search results include: video_id, video_title, text, start_time, end_time, speaker, score\n- Limit/offset pagination support\n\n## Implementation Notes\n- OpenSearch index creation should be idempotent (check if exists first)\n- Use opensearch-py bulk API for indexing segments\n- Hybrid query structure: use OpenSearch \"bool\" query with \"should\" clauses for BM25 + script_score for kNN\n- Alternatively, use OpenSearch native hybrid search if available in 2.11\n- Query embedding: use same embedding.generate_embedding() from embedding service\n- BM25 query: match on \"text\" field with english analyzer\n- kNN query: cosinesimil on \"embedding\" field\n- Result deduplication: if same segment appears in both BM25 and kNN results, take highest score\n- Add video_title to indexed document for display in search results\n- For testing: create test index with known documents, verify query returns expected results\n\n## Testing Requirements\n\n### Test Specification Reference\n- Document: docs/testing/phase1-test-specification.md\n- Sections: S1: Natural Language Search, S3: Timestamp Links in Results, C1 (partial - indexing)\n\n### Unit Tests\n- S1-U01: test_embedding_generation - BGE model generates 768-dim vector\n- S1-U02: test_hybrid_query_construction - BM25 + kNN query both present\n- S1-U03: test_search_result_ranking - Results sorted by score (highest first)\n- S1-U04: test_empty_query_handled - Empty string returns empty results\n\n### Integration Tests\n- S1-I01: test_search_finds_keyword_match - Query keyword returns matching segment\n- S1-I02: test_search_finds_semantic_match - Semantic query returns relevant segment\n- S1-I03: test_search_no_results - Nonsense query returns empty results\n- S1-I04: test_search_across_videos - Results from multiple indexed videos\n- S1-I05: test_opensearch_index_mapping - Index has correct schema (kNN enabled, 768-dim, english analyzer)\n- S3-I01: test_search_results_include_timestamps - start_time float present in results\n- S3-I02: test_search_results_include_video_id - video_id UUID present in results\n- C1-I02: test_chunks_indexed_to_opensearch - OpenSearch doc count matches DB segment count\n\n### E2E Scenarios\n- E2E-01 (steps 7-9): Search for key term, verify result found\n- E2E-04: Search with no results, verify empty handling\n\n### Quality Gate\nThis feature is NOT complete until:\n- All S1-U*, S1-I*, S3-I* tests pass\n- Hybrid search returns relevant results for both keyword and semantic queries\n- Index mapping correct with kNN enabled\n- Empty/nonsense queries handled gracefully\n- Search results include video_id, start_time, text, speaker\n\n## Reference Documentation\n- docs/design/data-model.md - OpenSearch segments_index mapping\n- docs/design/query-flow.md - Hybrid search architecture\n- docs/design/search-api.md - Search API endpoints\n- docs/implementation/phase1.md - OpenSearch index spec, search endpoints\n- docs/testing/phase1-test-specification.md - S1, S3, C1 test specs","status":"open","priority":2,"issue_type":"feature","created_at":"2026-02-07T13:56:35.991471432+02:00","updated_at":"2026-02-07T13:56:35.991471432+02:00"}
{"id":"w-wqm.1","title":"Plan tasks for Search \u0026 Indexing","description":"Read prompt/plan_tasks.txt for instructions. Then create implementation tasks for this feature.\n\nFeature ID: w-wqm\n\n## Context\nImplement OpenSearch indexing and hybrid search (BM25 + vector/kNN) backend. Includes index creation with proper mappings, bulk indexing of segments with embeddings, and the search API endpoint that accepts natural language queries and returns ranked results. Covers stories S1 and S3 (backend).\n\n## Sizing Guidance\nTasks should:\n- Modify 1-3 files\n- Read no more than 5-8 files for context\n- Have ONE clear deliverable\n- Be completable within ~50k tokens of context\n\n## Reference\nSee the feature description (bd show w-wqm) for full technical context.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T13:56:45.895328065+02:00","updated_at":"2026-02-07T13:56:45.895328065+02:00","dependencies":[{"issue_id":"w-wqm.1","depends_on_id":"w-wqm","type":"parent-child","created_at":"2026-02-07T13:56:45.899447046+02:00","created_by":"daemon"},{"issue_id":"w-wqm.1","depends_on_id":"w-pi2","type":"blocks","created_at":"2026-02-07T13:59:31.663341526+02:00","created_by":"daemon"}]}
{"id":"w-ych","title":"Library \u0026 Search UI","description":"# Feature: Library \u0026 Search UI\n\n## Overview\nImplement the video library view (home page) with video cards, filtering, sorting, and status badges, plus the search page with search bar, results display with timestamp links, and navigation to video playback. This completes the frontend experience for browsing and finding video content.\n\n## User Stories Covered\n- M1: Video library view with status/filtering\n- S1 (frontend): Search bar and query submission\n- S3 (frontend): Search results with clickable timestamp links\n- V3 (frontend): Status badges with distinct colors\n\n## Technical Context\n\n### Files to Create\n- `backend/app/api/routes/videos.py` - (extend) GET /api/videos (list with pagination, filtering, sorting)\n- `frontend/src/pages/LibraryPage.tsx` - Library page: video grid/list, filters, sort controls\n- `frontend/src/pages/SearchPage.tsx` - Search page: search bar, results list, loading state, no-results state\n- `frontend/src/components/library/VideoCard.tsx` - Video card: thumbnail, title, status badge, recording date, participants count\n- `frontend/src/components/library/VideoList.tsx` - Filterable/sortable video list container\n- `frontend/src/components/common/StatusBadge.tsx` - Status badge component with color coding: ready=green, processing/transcribing/chunking/indexing=yellow, error=red, uploaded=gray\n- `frontend/src/components/common/Layout.tsx` - Page layout with navigation header\n- `frontend/src/components/common/Navigation.tsx` - Navigation bar: Library, Upload, Search links\n- `frontend/src/components/search/SearchBar.tsx` - Search input with submit on Enter, loading state\n- `frontend/src/components/search/SearchResults.tsx` - Results list: video title, text snippet, timestamp link, speaker label\n- `frontend/src/api/search.ts` - API client: searchVideos(query, limit, offset) → SearchResponse\n- `frontend/src/__tests__/components/VideoCard.test.tsx` - Card rendering tests\n- `frontend/src/__tests__/pages/LibraryPage.test.tsx` - Library page tests\n- `frontend/src/__tests__/pages/SearchPage.test.tsx` - Search page tests\n- `frontend/src/__tests__/components/SearchBar.test.tsx` - Search bar tests\n- `backend/tests/integration/test_video_api.py` - (extend) List videos API tests\n\n### Files to Modify\n- `backend/app/api/routes/videos.py` - Add GET /api/videos with query params: status, sort, limit, offset\n- `frontend/src/App.tsx` - Wire up LibraryPage at / and /library, SearchPage at /search\n- `frontend/src/api/videos.ts` - Add listVideos(params) API call\n\n### Dependencies\n- Feature 1 (Scaffolding): Base app, routing, types\n- Feature 2 (Upload): Video records in DB\n- Feature 4 (Search): Search API backend endpoint\n- Feature 5 (Playback): VideoPage for navigation targets, TimestampLink component\n\n### Key Design Decisions\n- Library page is the home page (route: / and /library)\n- Video cards show thumbnail (or placeholder), title, recording_date, status badge, participant count\n- Filter by status: dropdown with \"All\", \"Ready\", \"Processing\", \"Error\"\n- Sort by: recording_date (default desc), title, created_at\n- Pagination: load more button or infinite scroll (simple offset-based)\n- Status badge colors: ready=green, processing/transcribing/chunking/indexing=yellow/amber, error=red, uploaded=gray\n- Search page: centered search bar, results below\n- Search results show: video title, text snippet (highlighted match), timestamp (formatted as MM:SS), speaker label\n- Clicking result timestamp navigates to /videos/{id}?t={start_time}\n- Empty search shows prompt text (\"Search across your video library...\")\n- No results shows friendly message\n- Loading state shows spinner\n- Navigation: always visible header with links to Library, Upload, Search\n\n## Implementation Notes\n- GET /api/videos query params: ?status=ready\u0026sort=recording_date\u0026order=desc\u0026limit=20\u0026offset=0\n- Backend uses SQLAlchemy query with optional filters and ordering\n- Frontend uses axios to call list/search APIs\n- VideoCard links to /videos/{id}\n- StatusBadge uses Tailwind classes for colors (bg-green-100 text-green-800, etc.)\n- SearchResults map over results, render TimestampLink for each\n- Use react-router-dom Link for navigation\n- Layout component wraps all pages with Navigation\n- data-testid attributes: library-page, video-card, status-badge-{status}, library-empty, search-input, search-loading, search-result, no-results, filter-status, sort-control\n\n## Testing Requirements\n\n### Test Specification Reference\n- Document: docs/testing/phase1-test-specification.md\n- Sections: M1: Video Library View, S1 (frontend), S3 (frontend), V3 (frontend)\n\n### Integration Tests (Backend)\n- M1-I01: test_list_videos_returns_all - GET /api/videos returns all videos\n- M1-I02: test_list_videos_pagination - limit/offset params work\n- M1-I03: test_list_videos_filter_by_status - ?status=ready filters correctly\n- M1-I04: test_list_videos_sort_by_date - ?sort=recording_date orders correctly\n\n### Frontend Unit Tests\n- M1-F01: test_library_page_renders - Video list displayed\n- M1-F02: test_video_card_displays_metadata - Title, date, status visible on card\n- M1-F03: test_filter_controls_present - Status filter dropdown present\n- M1-F04: test_sort_controls_present - Sort by date option present\n- M1-F05: test_thumbnail_displayed - \u003cimg\u003e with thumbnail src\n- V3-F01: test_status_badge_colors - Each status has distinct color\n- V3-F02: test_status_polling - Status auto-refreshes (polls every 5s on non-ready videos)\n- V3-F03: test_ready_notification - Badge changes to \"Ready\" when processing completes\n- S1-F01: test_search_bar_renders - Input field present\n- S1-F02: test_search_submits_query - Enter key calls API\n- S1-F03: test_search_loading_state - Spinner shown during search\n- S3-F01: test_search_result_shows_timestamp - \"at 1:23\" visible in result\n- S3-F02: test_search_result_link_navigates - Click pushes /videos/{id}?t=\n\n### E2E Scenarios\n- E2E-01 (steps 7-11): Search, see results, click timestamp, navigate to player\n- E2E-02 (step 1-2): Library → click video → player\n- E2E-03: Library filtering by status, sorting by date\n- E2E-04: Search with no results\n\n### Quality Gate\nThis feature is NOT complete until:\n- All M1-*, V3-F*, S1-F*, S3-F* tests pass\n- Library page displays video cards with thumbnails and status badges\n- Filtering and sorting work\n- Search returns results and navigates to video on click\n- Navigation between Library, Upload, Search works\n- Status badges show correct colors\n\n## Reference Documentation\n- docs/design/data-model.md - Video fields for display\n- docs/design/search-api.md - Search endpoint response format\n- docs/design/technology-stack.md - React, Tailwind, react-router versions\n- docs/implementation/phase1.md - Frontend pages, components, routes\n- docs/testing/phase1-test-specification.md - M1, S1, S3, V3 frontend test specs","status":"open","priority":2,"issue_type":"feature","created_at":"2026-02-07T13:58:36.080092251+02:00","updated_at":"2026-02-07T13:58:36.080092251+02:00"}
{"id":"w-ych.1","title":"Plan tasks for Library \u0026 Search UI","description":"Read prompt/plan_tasks.txt for instructions. Then create implementation tasks for this feature.\n\nFeature ID: w-ych\n\n## Context\nImplement the video library view (home page) with video cards, filtering, sorting, and status badges, plus the search page with search bar, results, and timestamp link navigation. Covers stories M1, S1 (frontend), S3 (frontend), V3 (frontend).\n\n## Sizing Guidance\nTasks should:\n- Modify 1-3 files\n- Read no more than 5-8 files for context\n- Have ONE clear deliverable\n- Be completable within ~50k tokens of context\n\n## Reference\nSee the feature description (bd show w-ych) for full technical context.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T13:58:48.840886494+02:00","updated_at":"2026-02-07T13:58:48.840886494+02:00","dependencies":[{"issue_id":"w-ych.1","depends_on_id":"w-ych","type":"parent-child","created_at":"2026-02-07T13:58:48.84422036+02:00","created_by":"daemon"},{"issue_id":"w-ych.1","depends_on_id":"w-jjb","type":"blocks","created_at":"2026-02-07T13:59:31.69245814+02:00","created_by":"daemon"}]}
{"id":"w-zqu","title":"Video Upload \u0026 Storage","description":"# Feature: Video Upload \u0026 Storage\n\n## Overview\nImplement the video upload flow end-to-end: backend API endpoint for receiving MKV files with metadata, file storage to disk, database record creation, Celery task dispatch, and the frontend upload form with progress indication. This is the entry point for all video content in the system.\n\n## User Stories Covered\n- V1: Upload MKV with metadata (title, date, participants, notes)\n- V3 (partial): Initial status set to \"uploaded\", status endpoint\n\n## Technical Context\n\n### Files to Create\n- `backend/app/api/routes/videos.py` - POST /api/videos (multipart upload), GET /api/videos/{id}, GET /api/videos/{id}/status\n- `backend/app/services/video.py` - Video CRUD: create_video (save file, create DB record, queue task), get_video, get_video_status\n- `frontend/src/pages/UploadPage.tsx` - Upload page with form and progress\n- `frontend/src/components/upload/UploadForm.tsx` - Form fields: file input (.mkv only), title, date, participants (comma-separated), context notes\n- `frontend/src/components/upload/UploadProgress.tsx` - Upload progress bar using axios onUploadProgress\n- `frontend/src/api/videos.ts` - API client: uploadVideo(), getVideo(), getVideoStatus()\n- `backend/tests/unit/test_video_schema.py` - Schema validation tests\n- `backend/tests/integration/test_video_api.py` - Upload API integration tests\n- `frontend/src/__tests__/components/UploadForm.test.tsx` - Frontend upload tests\n\n### Files to Modify\n- `backend/app/main.py` - Register video routes\n- `backend/app/api/routes/__init__.py` - Export video router\n- `frontend/src/App.tsx` - Wire up UploadPage route\n\n### Dependencies\n- Feature 1 (Project Scaffolding) must be complete - provides models, DB, config, Docker\n\n### Key Design Decisions\n- File upload via multipart/form-data (python-multipart)\n- Files saved to /data/videos/original/{uuid}.mkv\n- Participants parsed from comma-separated string to TEXT[] array\n- Upload endpoint returns video ID immediately, processing is async\n- File extension validation: only .mkv accepted (400 for others)\n- Title and recording_date are required fields (422 for missing)\n- After DB record creation, dispatch Celery task for processing (video_processing.delay(video_id))\n- Status endpoint supports polling from frontend\n\n## Implementation Notes\n- Upload endpoint must handle large files (streaming to disk, not memory)\n- Use UploadFile from FastAPI for file handling\n- Save file with UUID-based filename to avoid conflicts: {video_id}.mkv\n- Frontend UploadForm uses controlled inputs with React state\n- Progress bar uses axios upload progress callback\n- After successful upload, redirect to library or show success message\n- Participants field: split on comma, trim whitespace, store as array\n- data-testid attributes on form elements for E2E testing: upload-form, file-input, title-input, date-input, participants-input, submit-btn, progress-bar, success-message\n\n## Testing Requirements\n\n### Test Specification Reference\n- Document: docs/testing/phase1-test-specification.md\n- Sections: V1: Video Upload, V3: Processing Status (partial)\n\n### Unit Tests\n- V1-U01: test_video_schema_validation - Valid data passes, invalid rejected\n- V1-U02: test_video_file_extension_validation - Only .mkv accepted, non-MKV gets 400\n- V1-U03: test_video_metadata_required_fields - Missing title/date returns 422\n- V1-U04: test_participants_array_parsing - \"Alice, Bob\" stored as [\"Alice\", \"Bob\"]\n- V3-U01: test_status_enum_values - All expected status values defined\n\n### Integration Tests\n- V1-I01: test_upload_creates_video_record - POST /api/videos creates DB record with status=uploaded\n- V1-I02: test_upload_stores_file - File saved to /data/videos/original/\n- V1-I03: test_upload_triggers_processing_task - Celery task queued (mock Celery)\n- V1-I04: test_upload_returns_video_id - Response includes valid UUID\n- V3-I02: test_status_endpoint_returns_current - GET /api/videos/{id}/status returns status\n\n### Frontend Unit Tests\n- V1-F01: test_upload_form_renders - All fields visible\n- V1-F02: test_upload_form_validation - Error shown for missing title\n- V1-F03: test_file_type_restriction - File input accepts .mkv only\n- V1-F04: test_upload_progress_display - Progress bar updates\n\n### E2E Scenarios\n- E2E-01 (steps 1-5): Upload form, fill metadata, submit, observe progress\n- E2E-05: Upload corrupted video, observe error handling\n\n### Quality Gate\nThis feature is NOT complete until:\n- All V1-U*, V1-I*, V1-F* tests pass\n- V3-I02 (status endpoint) passes\n- Upload flow works manually via curl and via frontend\n- File lands on disk and DB record is created\n\n## Reference Documentation\n- docs/design/data-model.md - Video table schema\n- docs/design/processing-pipeline.md - Upload trigger flow\n- docs/implementation/phase1.md - API endpoints, project structure\n- docs/testing/phase1-test-specification.md - V1, V3 test specs","status":"open","priority":2,"issue_type":"feature","created_at":"2026-02-07T13:54:35.763931975+02:00","updated_at":"2026-02-07T13:54:35.763931975+02:00"}
{"id":"w-zqu.1","title":"Plan tasks for Video Upload \u0026 Storage","description":"Read prompt/plan_tasks.txt for instructions. Then create implementation tasks for this feature.\n\nFeature ID: w-zqu\n\n## Context\nImplement the video upload flow end-to-end: backend API endpoint for receiving MKV files with metadata, file storage, database record creation, Celery task dispatch, and the frontend upload form with progress indication. Covers user stories V1 and V3 (partial).\n\n## Sizing Guidance\nTasks should:\n- Modify 1-3 files\n- Read no more than 5-8 files for context\n- Have ONE clear deliverable\n- Be completable within ~50k tokens of context\n\n## Reference\nSee the feature description (bd show w-zqu) for full technical context.","status":"open","priority":2,"issue_type":"task","created_at":"2026-02-07T13:54:46.147122292+02:00","updated_at":"2026-02-07T13:54:46.147122292+02:00","dependencies":[{"issue_id":"w-zqu.1","depends_on_id":"w-zqu","type":"parent-child","created_at":"2026-02-07T13:54:46.151117391+02:00","created_by":"daemon"},{"issue_id":"w-zqu.1","depends_on_id":"w-7ib","type":"blocks","created_at":"2026-02-07T13:59:31.633733444+02:00","created_by":"daemon"}]}
